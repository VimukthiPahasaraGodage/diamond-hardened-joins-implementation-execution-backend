{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79807d8c-8858-4607-b81e-f77d60b857a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2041e55-c90f-414f-ada2-4155aff671b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sql_and_physical_plan(file_path):\n",
    "    result = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Split the content by the separator\n",
    "    entries = content.split('------------------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "    for entry in entries:\n",
    "        current_entry = {}\n",
    "        capture_mode = None\n",
    "        buffer = []\n",
    "        \n",
    "        lines = entry.splitlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "\n",
    "            if stripped_line.startswith(\"[SQL Query]\"):\n",
    "                # Save previous entry if complete\n",
    "                if 'SQL Query' in current_entry and 'Physical plan' in current_entry:\n",
    "                    result.append(current_entry)\n",
    "                current_entry = {}\n",
    "                capture_mode = 'SQL Query'\n",
    "                buffer = []\n",
    "                continue\n",
    "            elif stripped_line.startswith(\"[Physical plan]\"):\n",
    "                if capture_mode and buffer:\n",
    "                    current_entry[capture_mode] = '\\n'.join(buffer).strip()\n",
    "                capture_mode = 'Physical plan'\n",
    "                buffer = []\n",
    "                continue\n",
    "            elif stripped_line.startswith(\"[\") and stripped_line.endswith(\"]\"):\n",
    "                # Save current buffer if switching to a new section\n",
    "                if capture_mode and buffer:\n",
    "                    current_entry[capture_mode] = '\\n'.join(buffer).strip()\n",
    "                capture_mode = None\n",
    "                buffer = []\n",
    "                continue\n",
    "            \n",
    "            if capture_mode:\n",
    "                buffer.append(line.rstrip())\n",
    "\n",
    "        # Catch the last entry if needed\n",
    "        if capture_mode and buffer:\n",
    "            current_entry[capture_mode] = '\\n'.join(buffer).strip()\n",
    "        if 'SQL Query' in current_entry and 'Physical plan' in current_entry:\n",
    "            result.append(current_entry)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d5a28-7d0b-407c-b679-6e810e6bb6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'physical_plans.txt'\n",
    "data = extract_sql_and_physical_plan(file_path)\n",
    "\n",
    "for entry in data:\n",
    "    print(\"SQL Query:\\n\", entry['SQL Query'])\n",
    "    print(\"Physical plan:\\n\", entry['Physical plan'])\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fdf765-82a6-4ac5-bc62-d2d4f142b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class PlanNode:\n",
    "    op_type: str\n",
    "    attributes: Dict[str, str]\n",
    "    node_id: int\n",
    "    children: List[\"PlanNode\"] = field(default_factory=list)\n",
    "\n",
    "def parse_plan(plan_text: str) -> PlanNode:\n",
    "    lines = [line.rstrip() for line in plan_text.strip().split(\"\\n\") if line.strip()]\n",
    "    stack: List[Tuple[int, PlanNode]] = []\n",
    "\n",
    "    node_pattern = re.compile(r\"(\\w+)\\((.*?)\\), id\\s*=\\s*(\\d+)\")\n",
    "    attr_pattern = re.compile(r\"(\\w+)=\\[(.*?)\\]\")\n",
    "\n",
    "    for line in lines:\n",
    "        indent = len(line) - len(line.lstrip())\n",
    "        match = node_pattern.match(line.strip())\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        op_type, raw_attrs, node_id = match.groups()\n",
    "        node_id = int(node_id)\n",
    "\n",
    "        # Parse attributes\n",
    "        attributes = {k: v for k, v in attr_pattern.findall(raw_attrs)}\n",
    "\n",
    "        node = PlanNode(op_type, attributes, node_id)\n",
    "\n",
    "        # Attach to parent\n",
    "        while stack and stack[-1][0] >= indent:\n",
    "            stack.pop()\n",
    "        if stack:\n",
    "            stack[-1][1].children.append(node)\n",
    "\n",
    "        stack.append((indent, node))\n",
    "\n",
    "    # Root node is the first one in the stack\n",
    "    return stack[0][1] if stack else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b03df-5c1f-4105-b5be-863ae6e07a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Split on physical plan sections\n",
    "plans = text.split(\"Physical plan:\")\n",
    "for plan_text in plans[1:]:\n",
    "    root = parse_plan(plan_text)\n",
    "    print(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397ffb9-7781-42d1-9ed6-82cbbee88780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "from typing import Optional\n",
    "import html\n",
    "\n",
    "def escape_label(s):\n",
    "    return html.escape(str(s))\n",
    "\n",
    "def visualize_plan_tree(root: PlanNode, filename: Optional[str] = \"plan_tree\") -> Digraph:\n",
    "    dot = Digraph(format='png')\n",
    "    dot.attr(rankdir='TB')  # Top to bottom\n",
    "\n",
    "    def add_nodes_edges(node: PlanNode):\n",
    "        node_label = f\"{node.op_type}\\\\n(id={node.node_id})\"\n",
    "        for k, v in node.attributes.items():\n",
    "            node_label += f\"\\\\n{k}=[{v}]\"\n",
    "        node_label = escape_label(node_label)\n",
    "        dot.node(str(node.node_id), label=node_label, shape=\"box\", style=\"filled\", fillcolor=\"lightgray\")\n",
    "\n",
    "        for child in node.children:\n",
    "            dot.edge(str(node.node_id), str(child.node_id))\n",
    "            add_nodes_edges(child)\n",
    "\n",
    "    add_nodes_edges(root)\n",
    "    dot.render(filename, view=True)\n",
    "    return dot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ff4fe-71d7-4975-bd03-4f559cbfbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node = parse_plan(\"\"\"BindableAggregate(group=[{}], ALTERNATIVE_NAME=[MIN($0)], VOICED_CHAR_NAME=[MIN($1)], VOICING_ACTRESS=[MIN($2)], AMERICAN_MOVIE=[MIN($3)]), id = 37720614\n",
    "  BindableProject(name=[$2], name0=[$9], name2=[$35], title=[$46]), id = 37720613\n",
    "    BindableJoin(condition=[=($17, $45)], joinType=[inner]), id = 37720612\n",
    "      BindableJoin(condition=[=($21, $43)], joinType=[inner]), id = 37720611\n",
    "        BindableJoin(condition=[=($16, $34)], joinType=[inner]), id = 37720609\n",
    "          BindableJoin(condition=[AND(=($17, $30), =($31, $22))], joinType=[inner]), id = 37720607\n",
    "            BindableJoin(condition=[true], joinType=[inner]), id = 37720606\n",
    "              BindableProject(id=[$0], person_id=[$1], name=[$2], imdb_index=[$3], name_pcode_cf=[$4], name_pcode_nf=[$5], surname_pcode=[$6], md5sum=[$7], id1=[$15], name0=[$16], imdb_index0=[$17], imdb_id=[$18], name_pcode_nf0=[$19], surname_pcode0=[$20], md5sum0=[$21], id0=[$8], person_id0=[$9], movie_id=[$10], person_role_id=[$11], note=[$12], nr_order=[$13], role_id=[$14]), id = 37720604\n",
    "                BindableJoin(condition=[=($15, $11)], joinType=[inner]), id = 37720603\n",
    "                  BindableJoin(condition=[=($1, $9)], joinType=[inner]), id = 37720602\n",
    "                    BindableTableScan(table=[[aka_name]]), id = 37716643\n",
    "                    BindableFilter(condition=[SEARCH($4, Sarg['(voice)':VARCHAR, '(voice) (uncredited)':VARCHAR, '(voice: English version)':VARCHAR, '(voice: Japanese version)':VARCHAR]:VARCHAR)]), id = 37720601\n",
    "                      BindableTableScan(table=[[cast_info]]), id = 37716659\n",
    "                  BindableTableScan(table=[[char_name]]), id = 37716647\n",
    "              BindableFilter(condition=[=($2, '[us]')]), id = 37720605\n",
    "                BindableTableScan(table=[[company_name]]), id = 37716671\n",
    "            BindableTableScan(table=[[movie_companies]]), id = 37716683\n",
    "          BindableFilter(condition=[=($4, 'f')]), id = 37720608\n",
    "            BindableTableScan(table=[[name]]), id = 37716695\n",
    "        BindableFilter(condition=[=($1, 'actress')]), id = 37720610\n",
    "          BindableTableScan(table=[[role_type]]), id = 37716707\n",
    "      BindableTableScan(table=[[title]]), id = 37716719\n",
    "\"\"\")\n",
    "root_node\n",
    "visualize_plan_tree(root_node, filename=\"example_plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae4a6748-f622-45d6-a23f-676ec8aced05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from io import StringIO\n",
    "\n",
    "class PlanCodeGenerator:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.code_lines = []\n",
    "        self.visited = set()\n",
    "\n",
    "    def generate(self) -> str:\n",
    "        self.code_lines = [\n",
    "            \"import pandas as pd\",\n",
    "            \"# Load CSVs (you may need to adjust paths)\",\n",
    "        ]\n",
    "        self._gen_node_code(self.root)\n",
    "        self.code_lines.append(f\"\\nresult = df_{self.root.node_id}\")\n",
    "        self.code_lines.append(\"print(result.head())\")\n",
    "        return \"\\n\".join(self.code_lines)\n",
    "\n",
    "    def _gen_node_code(self, node):\n",
    "        if node.node_id in self.visited:\n",
    "            return\n",
    "        self.visited.add(node.node_id)\n",
    "\n",
    "        # Generate code for children first\n",
    "        for child in node.children:\n",
    "            self._gen_node_code(child)\n",
    "\n",
    "        op = node.op_type\n",
    "        df_name = f\"df_{node.node_id}\"\n",
    "\n",
    "        if op == \"BindableTableScan\":\n",
    "            table = self._extract_table_name(node.attributes.get(\"table\", \"\"))\n",
    "            self.code_lines.append(f\"{df_name} = pd.read_csv('{table}.csv')\")\n",
    "\n",
    "        elif op == \"BindableFilter\":\n",
    "            child_df = f\"df_{node.children[0].node_id}\"\n",
    "            condition = self._translate_condition(node.attributes.get(\"condition\", \"\"))\n",
    "            self.code_lines.append(f\"{df_name} = {child_df}.query({condition})\")\n",
    "\n",
    "        elif op == \"BindableProject\":\n",
    "            child_df = f\"df_{node.children[0].node_id}\"\n",
    "            cols = self._extract_projection_cols(node.attributes)\n",
    "            self.code_lines.append(f\"{df_name} = {child_df}[[{', '.join(cols)}]]\")\n",
    "\n",
    "        elif op == \"BindableJoin\":\n",
    "            left = f\"df_{node.children[0].node_id}\"\n",
    "            right = f\"df_{node.children[1].node_id}\"\n",
    "            join_type = node.attributes.get(\"joinType\", \"inner\").strip('[]')\n",
    "            on_cols = self._translate_join_condition(node.attributes.get(\"condition\", \"\"))\n",
    "            self.code_lines.append(f\"{df_name} = pd.merge({left}, {right}, how='{join_type}', {on_cols})\")\n",
    "\n",
    "        elif op == \"BindableAggregate\":\n",
    "            child_df = f\"df_{node.children[0].node_id}\"\n",
    "            aggs = self._translate_aggregates(node.attributes)\n",
    "            self.code_lines.append(f\"{df_name} = {child_df}.agg({aggs}).reset_index(drop=True)\")\n",
    "\n",
    "        elif op == \"BindableValues\":\n",
    "            self.code_lines.append(f\"{df_name} = pd.DataFrame([[]])\")\n",
    "\n",
    "        else:\n",
    "            self.code_lines.append(f\"# Unsupported node type: {op}\")\n",
    "            self.code_lines.append(f\"{df_name} = pd.DataFrame()  # placeholder\")\n",
    "\n",
    "    def _extract_table_name(self, table_attr):\n",
    "        # table=[[movie_companies]] => movie_companies\n",
    "        return table_attr.replace(\"[[\", \"\").replace(\"]]\", \"\").split(\".\")[-1]\n",
    "\n",
    "    def _extract_projection_cols(self, attrs: Dict[str, str]) -> List[str]:\n",
    "        # Extract projection columns like name=[$1], title=[$2] -> return ['\"name\"', '\"title\"']\n",
    "        return [f\"'{k}'\" for k in attrs.keys()]\n",
    "\n",
    "    def _translate_condition(self, cond: str) -> str:\n",
    "        # Minimal translator for now; could be improved with regex parsing\n",
    "        cond = cond.replace(\"LIKE\", \"str.contains\")\n",
    "        cond = cond.replace(\"AND\", \"and\").replace(\"OR\", \"or\")\n",
    "        cond = cond.replace(\"=\", \"==\")\n",
    "        cond = cond.replace(\"<>\", \"!=\")\n",
    "        cond = cond.replace(\"true\", \"True\").replace(\"false\", \"False\")\n",
    "        return f'\"\"\" {cond} \"\"\"'\n",
    "\n",
    "    def _translate_join_condition(self, cond: str) -> str:\n",
    "        # Example: =($0, $1) => on=['col0', 'col1']\n",
    "        matches = re.findall(r\"\\$([0-9]+)\", cond)\n",
    "        if len(matches) == 2:\n",
    "            return f\"left_on='col{matches[0]}', right_on='col{matches[1]}'\"\n",
    "        elif \"AND\" in cond:\n",
    "            cols = re.findall(r\"\\$([0-9]+)\", cond)\n",
    "            return f\"left_on={[f'col{c}' for c in cols[::2]]}, right_on={[f'col{c}' for c in cols[1::2]]}\"\n",
    "        else:\n",
    "            return \"# TODO: Complex join condition\"\n",
    "\n",
    "    def _translate_aggregates(self, attrs: Dict[str, str]) -> str:\n",
    "        agg_map = {}\n",
    "        for k, v in attrs.items():\n",
    "            match = re.search(r'MIN\\(\\$(\\d+)\\)', v)\n",
    "            if match:\n",
    "                col = f\"col{match.group(1)}\"\n",
    "                agg_map[k.lower()] = f\"('{col}', 'min')\"\n",
    "            # You can add other aggregate support here (e.g. MAX, SUM, etc.)\n",
    "    \n",
    "        return \"{\" + \", \".join(f\"'{k}': {v}\" for k, v in agg_map.items()) + \"}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "375bdcec-6bdb-49b4-8199-e9653c18d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "# Load CSVs (you may need to adjust paths)\n",
      "df_37716643 = pd.read_csv('[aka_name.csv')\n",
      "df_37716659 = pd.read_csv('[cast_info.csv')\n",
      "df_37720601 = df_37716659.query(\"\"\" SEARCH($4, Sarg['(voice)':VARCHAR, '(voice) (uncredited)':VARCHAR, '(voice: English version)':VARCHAR, '(voice: Japanese version)':VARCHAR \"\"\")\n",
      "df_37720602 = pd.merge(df_37716643, df_37720601, how='inner', left_on='col1', right_on='col9')\n",
      "df_37716647 = pd.read_csv('[char_name.csv')\n",
      "df_37720603 = pd.merge(df_37720602, df_37716647, how='inner', left_on='col15', right_on='col11')\n",
      "df_37720604 = df_37720603[['id', 'person_id', 'name', 'imdb_index', 'name_pcode_cf', 'name_pcode_nf', 'surname_pcode', 'md5sum', 'id1', 'name0', 'imdb_index0', 'imdb_id', 'name_pcode_nf0', 'surname_pcode0', 'md5sum0', 'id0', 'person_id0', 'movie_id', 'person_role_id', 'note', 'nr_order', 'role_id']]\n",
      "df_37716671 = pd.read_csv('[company_name.csv')\n",
      "df_37720605 = df_37716671.query(\"\"\" ==($2, '[us \"\"\")\n",
      "df_37720606 = pd.merge(df_37720604, df_37720605, how='inner', # TODO: Complex join condition)\n",
      "df_37716683 = pd.read_csv('[movie_companies.csv')\n",
      "df_37720607 = pd.merge(df_37720606, df_37716683, how='inner', left_on=['col17', 'col31'], right_on=['col30', 'col22'])\n",
      "df_37716695 = pd.read_csv('[name.csv')\n",
      "df_37720608 = df_37716695.query(\"\"\" ==($4, 'f') \"\"\")\n",
      "df_37720609 = pd.merge(df_37720607, df_37720608, how='inner', left_on='col16', right_on='col34')\n",
      "df_37716707 = pd.read_csv('[role_type.csv')\n",
      "df_37720610 = df_37716707.query(\"\"\" ==($1, 'actress') \"\"\")\n",
      "df_37720611 = pd.merge(df_37720609, df_37720610, how='inner', left_on='col21', right_on='col43')\n",
      "df_37716719 = pd.read_csv('[title.csv')\n",
      "df_37720612 = pd.merge(df_37720611, df_37716719, how='inner', left_on='col17', right_on='col45')\n",
      "df_37720613 = df_37720612[['name', 'name0', 'name2', 'title']]\n",
      "df_37720614 = df_37720613.agg({'alternative_name': ('col0', 'min'), 'voiced_char_name': ('col1', 'min'), 'voicing_actress': ('col2', 'min'), 'american_movie': ('col3', 'min')}).reset_index(drop=True)\n",
      "\n",
      "result = df_37720614\n",
      "print(result.head())\n"
     ]
    }
   ],
   "source": [
    "plan_text = \"\"\"BindableAggregate(group=[{}], ALTERNATIVE_NAME=[MIN($0)], VOICED_CHAR_NAME=[MIN($1)], VOICING_ACTRESS=[MIN($2)], AMERICAN_MOVIE=[MIN($3)]), id = 37720614\n",
    "  BindableProject(name=[$2], name0=[$9], name2=[$35], title=[$46]), id = 37720613\n",
    "    BindableJoin(condition=[=($17, $45)], joinType=[inner]), id = 37720612\n",
    "      BindableJoin(condition=[=($21, $43)], joinType=[inner]), id = 37720611\n",
    "        BindableJoin(condition=[=($16, $34)], joinType=[inner]), id = 37720609\n",
    "          BindableJoin(condition=[AND(=($17, $30), =($31, $22))], joinType=[inner]), id = 37720607\n",
    "            BindableJoin(condition=[true], joinType=[inner]), id = 37720606\n",
    "              BindableProject(id=[$0], person_id=[$1], name=[$2], imdb_index=[$3], name_pcode_cf=[$4], name_pcode_nf=[$5], surname_pcode=[$6], md5sum=[$7], id1=[$15], name0=[$16], imdb_index0=[$17], imdb_id=[$18], name_pcode_nf0=[$19], surname_pcode0=[$20], md5sum0=[$21], id0=[$8], person_id0=[$9], movie_id=[$10], person_role_id=[$11], note=[$12], nr_order=[$13], role_id=[$14]), id = 37720604\n",
    "                BindableJoin(condition=[=($15, $11)], joinType=[inner]), id = 37720603\n",
    "                  BindableJoin(condition=[=($1, $9)], joinType=[inner]), id = 37720602\n",
    "                    BindableTableScan(table=[[aka_name]]), id = 37716643\n",
    "                    BindableFilter(condition=[SEARCH($4, Sarg['(voice)':VARCHAR, '(voice) (uncredited)':VARCHAR, '(voice: English version)':VARCHAR, '(voice: Japanese version)':VARCHAR]:VARCHAR)]), id = 37720601\n",
    "                      BindableTableScan(table=[[cast_info]]), id = 37716659\n",
    "                  BindableTableScan(table=[[char_name]]), id = 37716647\n",
    "              BindableFilter(condition=[=($2, '[us]')]), id = 37720605\n",
    "                BindableTableScan(table=[[company_name]]), id = 37716671\n",
    "            BindableTableScan(table=[[movie_companies]]), id = 37716683\n",
    "          BindableFilter(condition=[=($4, 'f')]), id = 37720608\n",
    "            BindableTableScan(table=[[name]]), id = 37716695\n",
    "        BindableFilter(condition=[=($1, 'actress')]), id = 37720610\n",
    "          BindableTableScan(table=[[role_type]]), id = 37716707\n",
    "      BindableTableScan(table=[[title]]), id = 37716719\"\"\"\n",
    "root = parse_plan(plan_text)\n",
    "generator = PlanCodeGenerator(root)\n",
    "code = generator.generate()\n",
    "\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fe85d8-d317-4903-bea2-c30859cdca2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R1_a  R2_a R2_b R3_b  R3_c\n",
      "0     1     1    x    x   100\n",
      "1     2     2    y    y   200\n",
      "2     2     2    y    y   200\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# example DataFrames\n",
    "# R1 has column 'a'\n",
    "# R2 has columns 'a', 'b'\n",
    "# R3 has column 'b'\n",
    "# replace these with your actual DataFrames\n",
    "df1 = pd.DataFrame({'a': [1,2,2,3]})\n",
    "df2 = pd.DataFrame({'a': [1,2,2,4], 'b': ['x','y','z','y']})\n",
    "df3 = pd.DataFrame({'b': ['x','y','w'], 'c': [100,200,300]})\n",
    "\n",
    "# 1) build hash table for R2 on key = a\n",
    "ht2 = defaultdict(list)\n",
    "for _, row in df2.iterrows():\n",
    "    ht2[row['a']].append(row)\n",
    "\n",
    "# 2) build hash table for R3 on key = b\n",
    "ht3 = defaultdict(list)\n",
    "for _, row in df3.iterrows():\n",
    "    ht3[row['b']].append(row)\n",
    "\n",
    "# 3) nested‐iterator join\n",
    "results = []\n",
    "for _, r1 in df1.iterrows():\n",
    "    # find all R2 rows where R2.a == R1.a\n",
    "    for r2 in ht2.get(r1['a'], []):\n",
    "        # for each, find all R3 rows where R3.b == R2.b\n",
    "        for r3 in ht3.get(r2['b'], []):\n",
    "            # combine the three rows into one result row.\n",
    "            # here we prefix columns to avoid name clashes.\n",
    "            out = {}\n",
    "            out.update({f\"R1_{col}\": r1[col] for col in df1.columns})\n",
    "            out.update({f\"R2_{col}\": r2[col] for col in df2.columns})\n",
    "            out.update({f\"R3_{col}\": r3[col] for col in df3.columns})\n",
    "            results.append(out)\n",
    "\n",
    "# 4) materialize as a DataFrame\n",
    "joined_df = pd.DataFrame(results)\n",
    "\n",
    "print(joined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b7b99a9-1c4b-4212-9881-36b6ddefe9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R1_a</th>\n",
       "      <th>R2_a</th>\n",
       "      <th>R2_b</th>\n",
       "      <th>R3_b</th>\n",
       "      <th>R3_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R1_a  R2_a R2_b R3_b  R3_c\n",
       "0     2     2    y    y   200\n",
       "1     2     2    y    y   200\n",
       "2     1     1    x    x   100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def build_ht(df, key):\n",
    "    ht = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        ht[row[key]].append(row)\n",
    "    return ht\n",
    "\n",
    "def join_chunk(chunk_df1, ht2, ht3):\n",
    "    out = []\n",
    "    for _, r1 in chunk_df1.iterrows():\n",
    "        for r2 in ht2.get(r1['a'], ()):\n",
    "            for r3 in ht3.get(r2['b'], ()):\n",
    "                row = {f\"R1_{c}\": r1[c] for c in chunk_df1.columns}\n",
    "                row.update({f\"R2_{c}\": r2[c] for c in r2.index})\n",
    "                row.update({f\"R3_{c}\": r3[c] for c in r3.index})\n",
    "                out.append(row)\n",
    "    return out\n",
    "\n",
    "# Pre-build once\n",
    "ht2 = build_ht(df2, 'a')\n",
    "ht3 = build_ht(df3, 'b')\n",
    "\n",
    "# Split R1 into N roughly even chunks\n",
    "num_threads = 4\n",
    "chunks = np.array_split(df1, num_threads)\n",
    "\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=num_threads) as exe:\n",
    "    futures = [exe.submit(join_chunk, chunk, ht2, ht3) for chunk in chunks]\n",
    "    for f in as_completed(futures):\n",
    "        results.extend(f.result())\n",
    "\n",
    "joined = pd.DataFrame(results)\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3248f789-3aaa-403d-bcf3-7cc4af201466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37e86d46-66b9-4bdf-8bbf-01975fbcbf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Timing ===\n",
      " Total elapsed       : 71.434 s\n",
      "  - Hash-build phase : 0.024 s\n",
      "  - Join phase       : 71.410 s\n",
      "\n",
      "=== Output ===\n",
      " Total tuples produced: 99916364\n",
      "\n",
      "=== Memory (RSS) ===\n",
      " Before              : 186.7 MB\n",
      " After build         : 188.2 MB\n",
      " After join          : 7838.2 MB\n",
      " -------------------------\n",
      " Memory for ht build : 1.5 MB\n",
      " Memory for join     : 7650.0 MB\n",
      " Total bump          : 7651.5 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Example DataFrames (replace with your real ones)\n",
    "df1 = pd.DataFrame({'a': np.random.randint(0, 100, size=10000)})\n",
    "df2 = pd.DataFrame({\n",
    "    'a': np.random.randint(0, 100, size=10000),\n",
    "    'b': np.random.randint(0, 100, size=10000)\n",
    "})\n",
    "df3 = pd.DataFrame({\n",
    "    'b': np.random.randint(0, 100, size=10000),\n",
    "    'c': np.random.randn(10000)\n",
    "})\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "\n",
    "def mem_mb():\n",
    "    return proc.memory_info().rss / 1024**2\n",
    "\n",
    "def build_ht(df, key):\n",
    "    ht = defaultdict(list)\n",
    "    for row in df.itertuples(index=False):\n",
    "        ht[getattr(row, key)].append(row)\n",
    "    return ht\n",
    "\n",
    "def join_chunk(chunk_df1, ht2, ht3):\n",
    "    out = []\n",
    "    for r1 in chunk_df1.itertuples(index=False):\n",
    "        for r2 in ht2.get(r1.a, ()):\n",
    "            for r3 in ht3.get(r2.b, ()):\n",
    "                out.append((r1, r2, r3))\n",
    "    return out\n",
    "\n",
    "def parallel_hash_join(df1, df2, df3, num_threads=4):\n",
    "    # 1) record mem before anything\n",
    "    mem_before = mem_mb()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # 2) build hash tables\n",
    "    ht2 = build_ht(df2, 'a')\n",
    "    ht3 = build_ht(df3, 'b')\n",
    "    t1 = time.perf_counter()\n",
    "    mem_after_build = mem_mb()\n",
    "\n",
    "    # 3) split and dispatch\n",
    "    chunks = np.array_split(df1, num_threads)\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as exe:\n",
    "        futures = [exe.submit(join_chunk, chunk, ht2, ht3) for chunk in chunks]\n",
    "        for f in as_completed(futures):\n",
    "            results.extend(f.result())\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    mem_after_join = mem_mb()\n",
    "\n",
    "    # 4) stats\n",
    "    total_time = t2 - t0\n",
    "    build_time = t1 - t0\n",
    "    join_time  = t2 - t1\n",
    "    n_tuples   = len(results)\n",
    "    ht_memory  = mem_after_build - mem_before\n",
    "    join_memory = mem_after_join - mem_after_build\n",
    "    total_memory = mem_after_join - mem_before\n",
    "\n",
    "    # 5) report\n",
    "    print(f\"=== Timing ===\")\n",
    "    print(f\" Total elapsed       : {total_time:.3f} s\")\n",
    "    print(f\"  - Hash-build phase : {build_time :.3f} s\")\n",
    "    print(f\"  - Join phase       : {join_time  :.3f} s\")\n",
    "    print()\n",
    "    print(f\"=== Output ===\")\n",
    "    print(f\" Total tuples produced: {n_tuples}\")\n",
    "    print()\n",
    "    print(f\"=== Memory (RSS) ===\")\n",
    "    print(f\" Before              : {mem_before   :.1f} MB\")\n",
    "    print(f\" After build         : {mem_after_build:.1f} MB\")\n",
    "    print(f\" After join          : {mem_after_join :.1f} MB\")\n",
    "    print(f\" -------------------------\")\n",
    "    print(f\" Memory for ht build : {ht_memory  :.1f} MB\")\n",
    "    print(f\" Memory for join     : {join_memory:.1f} MB\")\n",
    "    print(f\" Total bump          : {total_memory:.1f} MB\")\n",
    "\n",
    "    return pd.DataFrame(results,\n",
    "                        columns=['R1_row','R2_row','R3_row'])\n",
    "\n",
    "# Run it\n",
    "joined_df = parallel_hash_join(df1, df2, df3, num_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efb5e3d0-a43b-40c8-b59e-0deca0270e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Timing ===\n",
      " Total elapsed       : 60.011 s\n",
      "  - Hash-build phase : 0.044 s\n",
      "  - Join phase       : 59.966 s\n",
      "\n",
      "=== Output ===\n",
      " Total tuples produced: 100221691\n",
      "\n",
      "=== Memory (RSS) ===\n",
      " Before              : 126.1 MB\n",
      " After build         : 128.6 MB\n",
      " After join          : 7801.6 MB\n",
      " -------------------------\n",
      " Memory for ht build : 2.5 MB\n",
      " Memory for join     : 7673.0 MB\n",
      " Total bump          : 7675.5 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Example DataFrames (replace with your real ones)\n",
    "df1 = pd.DataFrame({'a': np.random.randint(0, 100, size=10000)})\n",
    "df2 = pd.DataFrame({\n",
    "    'a': np.random.randint(0, 100, size=10000),\n",
    "    'b': np.random.randint(0, 100, size=10000)\n",
    "})\n",
    "df3 = pd.DataFrame({\n",
    "    'b': np.random.randint(0, 100, size=10000),\n",
    "    'c': np.random.randn(10000)\n",
    "})\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "\n",
    "def build_ht_chunk(rows, key):\n",
    "    \"\"\"Build a partial hash‐table from an iterable of rows.\"\"\"\n",
    "    partial = defaultdict(list)\n",
    "    for r in rows:\n",
    "        partial[getattr(r, key)].append(r)\n",
    "    return partial\n",
    "\n",
    "def merge_hash_tables(dicts):\n",
    "    \"\"\"Merge a list of defaultdict(list) into one.\"\"\"\n",
    "    ht = defaultdict(list)\n",
    "    for part in dicts:\n",
    "        for k, lst in part.items():\n",
    "            ht[k].extend(lst)\n",
    "    return ht\n",
    "\n",
    "def parallel_build_ht(df, key, num_workers=4):\n",
    "    # 1) split the DataFrame into DataFrame‐chunks (not iterators)\n",
    "    df_chunks = np.array_split(df, num_workers)\n",
    "\n",
    "    # 2) in each thread, build a partial hash‐table over its chunk\n",
    "    partials = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as exe:\n",
    "        futures = [\n",
    "            exe.submit(build_ht_chunk, chunk.itertuples(index=False), key)\n",
    "            for chunk in df_chunks\n",
    "        ]\n",
    "        for f in as_completed(futures):\n",
    "            partials.append(f.result())\n",
    "\n",
    "    # 3) merge them into one big hash‐table\n",
    "    return merge_hash_tables(partials)\n",
    "\n",
    "def mem_mb():\n",
    "    return proc.memory_info().rss / 1024**2\n",
    "\n",
    "def build_ht(df, key):\n",
    "    ht = defaultdict(list)\n",
    "    for row in df.itertuples(index=False):\n",
    "        ht[getattr(row, key)].append(row)\n",
    "    return ht\n",
    "\n",
    "def join_chunk(chunk_df1, ht2, ht3):\n",
    "    out = []\n",
    "    for r1 in chunk_df1.itertuples(index=False):\n",
    "        for r2 in ht2.get(r1.a, ()):\n",
    "            for r3 in ht3.get(r2.b, ()):\n",
    "                out.append((r1, r2, r3))\n",
    "    return out\n",
    "\n",
    "def parallel_hash_join(df1, df2, df3, num_threads=4):\n",
    "    # 1) record mem before anything\n",
    "    mem_before = mem_mb()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # 2) build hash tables\n",
    "    ht2 = parallel_build_ht(df2, 'a', num_workers=4)\n",
    "    ht3 = parallel_build_ht(df3, 'b', num_workers=4)\n",
    "    t1 = time.perf_counter()\n",
    "    mem_after_build = mem_mb()\n",
    "\n",
    "    # 3) split and dispatch\n",
    "    chunks = np.array_split(df1, num_threads)\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as exe:\n",
    "        futures = [exe.submit(join_chunk, chunk, ht2, ht3) for chunk in chunks]\n",
    "        for f in as_completed(futures):\n",
    "            results.extend(f.result())\n",
    "\n",
    "    t2 = time.perf_counter()\n",
    "    mem_after_join = mem_mb()\n",
    "\n",
    "    # 4) stats\n",
    "    total_time = t2 - t0\n",
    "    build_time = t1 - t0\n",
    "    join_time  = t2 - t1\n",
    "    n_tuples   = len(results)\n",
    "    ht_memory  = mem_after_build - mem_before\n",
    "    join_memory = mem_after_join - mem_after_build\n",
    "    total_memory = mem_after_join - mem_before\n",
    "\n",
    "    # 5) report\n",
    "    print(f\"=== Timing ===\")\n",
    "    print(f\" Total elapsed       : {total_time:.3f} s\")\n",
    "    print(f\"  - Hash-build phase : {build_time :.3f} s\")\n",
    "    print(f\"  - Join phase       : {join_time  :.3f} s\")\n",
    "    print()\n",
    "    print(f\"=== Output ===\")\n",
    "    print(f\" Total tuples produced: {n_tuples}\")\n",
    "    print()\n",
    "    print(f\"=== Memory (RSS) ===\")\n",
    "    print(f\" Before              : {mem_before   :.1f} MB\")\n",
    "    print(f\" After build         : {mem_after_build:.1f} MB\")\n",
    "    print(f\" After join          : {mem_after_join :.1f} MB\")\n",
    "    print(f\" -------------------------\")\n",
    "    print(f\" Memory for ht build : {ht_memory  :.1f} MB\")\n",
    "    print(f\" Memory for join     : {join_memory:.1f} MB\")\n",
    "    print(f\" Total bump          : {total_memory:.1f} MB\")\n",
    "\n",
    "    return pd.DataFrame(results,\n",
    "                        columns=['R1_row','R2_row','R3_row'])\n",
    "\n",
    "# Run it\n",
    "joined_df = parallel_hash_join(df1, df2, df3, num_threads=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813e301b-ac55-4ee6-9ec7-f2db4b69368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b\n",
      "0  1  x\n",
      "1  2  y\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrames (replace with your actual data)\n",
    "R1 = pd.DataFrame({'a': [1, 2, 3]})\n",
    "R2 = pd.DataFrame({'a': [1, 2, 4], 'b': ['x', 'y', 'z']})\n",
    "R3 = pd.DataFrame({'b': ['x', 'y', 'w']})\n",
    "\n",
    "def hash_join(df1, df2, key1, key2=None, how='inner'):\n",
    "    \"\"\"Performs a hash join between df1 and df2 on given keys.\"\"\"\n",
    "    if key2 is None:\n",
    "        key2 = key1\n",
    "    hash_table = {}\n",
    "    # Build phase on df2\n",
    "    for _, row in df2.iterrows():\n",
    "        key = row[key2]\n",
    "        if key not in hash_table:\n",
    "            hash_table[key] = []\n",
    "        hash_table[key].append(row)\n",
    "    \n",
    "    result_rows = []\n",
    "    # Probe phase on df1\n",
    "    for _, row1 in df1.iterrows():\n",
    "        key = row1[key1]\n",
    "        if key in hash_table:\n",
    "            for row2 in hash_table[key]:\n",
    "                combined = pd.concat([row1, row2.drop(labels=key2)], axis=0)\n",
    "                result_rows.append(combined)\n",
    "    \n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "# Step 1: Join R2 and R3 on 'b'\n",
    "R2_R3_joined = hash_join(R2, R3, key1='b')\n",
    "\n",
    "# Step 2: Join the result with R1 on 'a'\n",
    "final_result = hash_join(R1, R2_R3_joined, key1='a')\n",
    "\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c5b549-7e02-4598-8a8a-23aec0412036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Timing ===\n",
      " Total elapsed       : 4.913 s\n",
      "  - Hash-build phase : 0.034 s\n",
      "  - Join phase       : 4.879 s\n",
      "\n",
      "=== Output ===\n",
      " Total tuples produced: 4\n",
      "\n",
      "=== Memory (RSS) ===\n",
      " Before              : 125.3 MB\n",
      " After build         : 126.8 MB\n",
      " After join          : 153.2 MB\n",
      " -------------------------\n",
      " Memory for ht build : 1.5 MB\n",
      " Memory for join     : 26.4 MB\n",
      " Total bump          : 27.9 MB\n",
      "=== Timing ===\n",
      " Total elapsed       : 46.411 s\n",
      "  - Hash-build phase : 0.365 s\n",
      "  - Join phase       : 46.046 s\n",
      "\n",
      "=== Output ===\n",
      " Total tuples produced: 4\n",
      "\n",
      "=== Memory (RSS) ===\n",
      " Before              : 148.2 MB\n",
      " After build         : 148.2 MB\n",
      " After join          : 308.2 MB\n",
      " -------------------------\n",
      " Memory for ht build : 0.0 MB\n",
      " Memory for join     : 160.0 MB\n",
      " Total bump          : 160.0 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.436417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.238719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.595987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.476777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.165180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99718</th>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-0.444299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99719</th>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-1.149624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99720</th>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>-1.261386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99721</th>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1.303021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99722</th>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.340194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99723 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          a     b         c\n",
       "0      23.0   5.0 -0.436417\n",
       "1      23.0   5.0 -0.238719\n",
       "2      23.0   5.0 -0.595987\n",
       "3      23.0   5.0  0.476777\n",
       "4      23.0   5.0 -0.165180\n",
       "...     ...   ...       ...\n",
       "99718  28.0  33.0 -0.444299\n",
       "99719  28.0  33.0 -1.149624\n",
       "99720  28.0  33.0 -1.261386\n",
       "99721  28.0  33.0  1.303021\n",
       "99722  28.0  33.0  0.340194\n",
       "\n",
       "[99723 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Sample data\n",
    "R1 = pd.DataFrame({'a': np.random.randint(0, 100, size=1000)})\n",
    "R2 = pd.DataFrame({\n",
    "    'a': np.random.randint(0, 100, size=1000),\n",
    "    'b': np.random.randint(0, 100, size=1000)\n",
    "})\n",
    "R3 = pd.DataFrame({\n",
    "    'b': np.random.randint(0, 100, size=1000),\n",
    "    'c': np.random.randn(1000)\n",
    "})\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "\n",
    "def build_hash_table(df, key):\n",
    "    \"\"\"Builds hash table on key column from df.\"\"\"\n",
    "    hash_table = {}\n",
    "    for _, row in df.iterrows():\n",
    "        k = row[key]\n",
    "        if k not in hash_table:\n",
    "            hash_table[k] = []\n",
    "        hash_table[k].append(row)\n",
    "    return hash_table\n",
    "\n",
    "def mem_mb():\n",
    "    return proc.memory_info().rss / 1024**2\n",
    "\n",
    "def probe_partition(df_partition, hash_table, key1, key2):\n",
    "    \"\"\"Probe a partition of df against a hash table.\"\"\"\n",
    "    result_rows = []\n",
    "    for _, row1 in df_partition.iterrows():\n",
    "        key = row1[key1]\n",
    "        if key in hash_table:\n",
    "            for row2 in hash_table[key]:\n",
    "                combined = pd.concat([row1, row2.drop(labels=key2)], axis=0)\n",
    "                result_rows.append(combined)\n",
    "    return pd.DataFrame(result_rows)\n",
    "\n",
    "def parallel_hash_join(df1, df2, key1, key2=None, threads=4):\n",
    "    if key2 is None:\n",
    "        key2 = key1\n",
    "\n",
    "    mem_before = mem_mb()\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # Build phase on df2\n",
    "    hash_table = build_hash_table(df2, key2)\n",
    "    t1 = time.perf_counter()\n",
    "    mem_after_build = mem_mb()\n",
    "\n",
    "    # Partition df1 into chunks\n",
    "    partitions = [df1.iloc[i::threads] for i in range(threads)]\n",
    "\n",
    "    # Parallel probe phase\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "        futures = [executor.submit(probe_partition, part, hash_table, key1, key2)\n",
    "                   for part in partitions]\n",
    "        for future in as_completed(futures):\n",
    "            results.append(future.result())\n",
    "\n",
    "    result = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    t2 = time.perf_counter()\n",
    "    mem_after_join = mem_mb()\n",
    "\n",
    "    # 4) stats\n",
    "    total_time = t2 - t0\n",
    "    build_time = t1 - t0\n",
    "    join_time  = t2 - t1\n",
    "    n_tuples   = len(results)\n",
    "    ht_memory  = mem_after_build - mem_before\n",
    "    join_memory = mem_after_join - mem_after_build\n",
    "    total_memory = mem_after_join - mem_before\n",
    "\n",
    "    # 5) report\n",
    "    print(f\"=== Timing ===\")\n",
    "    print(f\" Total elapsed       : {total_time:.3f} s\")\n",
    "    print(f\"  - Hash-build phase : {build_time :.3f} s\")\n",
    "    print(f\"  - Join phase       : {join_time  :.3f} s\")\n",
    "    print()\n",
    "    print(f\"=== Output ===\")\n",
    "    print(f\" Total tuples produced: {n_tuples}\")\n",
    "    print()\n",
    "    print(f\"=== Memory (RSS) ===\")\n",
    "    print(f\" Before              : {mem_before   :.1f} MB\")\n",
    "    print(f\" After build         : {mem_after_build:.1f} MB\")\n",
    "    print(f\" After join          : {mem_after_join :.1f} MB\")\n",
    "    print(f\" -------------------------\")\n",
    "    print(f\" Memory for ht build : {ht_memory  :.1f} MB\")\n",
    "    print(f\" Memory for join     : {join_memory:.1f} MB\")\n",
    "    print(f\" Total bump          : {total_memory:.1f} MB\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# Join R2 and R3 on 'b'\n",
    "R2_R3_joined = parallel_hash_join(R2, R3, key1='b', threads=4)\n",
    "\n",
    "# Join result with R1 on 'a'\n",
    "final_result = parallel_hash_join(R1, R2_R3_joined, key1='a', threads=4)\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55346dcb-b1f5-4b05-9b3e-b820bb8a68fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group by: []\n",
      "agg map: {'UNCREDITED_VOICED_CHARACTER': ('uncited_voiced_character', 'min'), 'RUSSIAN_MOVIE': ('russian_movie', 'min')}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_bindable_aggregate(line, input_cols):\n",
    "    \"\"\"\n",
    "    Parse a single BindableAggregate spec and return\n",
    "    (group_cols, agg_dict) where\n",
    "      - group_cols is a list of column names to group by,\n",
    "      - agg_dict is the dict you can pass to pandas .agg().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    line : str\n",
    "        e.g. \"BindableAggregate(group=[{COMPANY}], FROM_COMPANY=[MIN($0)], LINK_TYPE=[MIN($1)], SEQUEL_MOVIE=[MIN($2)])\"\n",
    "    input_cols : List[str]\n",
    "        e.g. ['from_company', 'link_type', 'sequel_movie']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    group_cols : List[str]\n",
    "    agg_dict : Dict[str, Tuple[str,str]]\n",
    "    \"\"\"\n",
    "    # 1) pull out group list\n",
    "    grp_match = re.search(r'group=\\[\\{(.*?)\\}\\]', line)\n",
    "    if grp_match and grp_match.group(1).strip():\n",
    "        group_cols = [col.strip() for col in grp_match.group(1).split(',')]\n",
    "    else:\n",
    "        group_cols = []\n",
    "    \n",
    "    # 2) pull every output=[FUNC($i)] chunk\n",
    "    pairs = re.findall(r'(\\w+)=\\[\\s*(\\w+)\\s*\\(\\s*\\$(\\d+)\\s*\\)\\s*\\]', line)\n",
    "    \n",
    "    # 3) build the agg dict\n",
    "    agg_dict = {}\n",
    "    for out_col, func, idx in pairs:\n",
    "        i = int(idx)\n",
    "        try:\n",
    "            in_col = input_cols[i]\n",
    "        except IndexError:\n",
    "            raise ValueError(f\"No input column at position ${i}\")\n",
    "        agg_dict[out_col] = (in_col, func.lower())\n",
    "    \n",
    "    return group_cols, agg_dict\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "line = (\n",
    "    \"BindableAggregate(group=[{}], \"\n",
    "    \"UNCREDITED_VOICED_CHARACTER=[MIN($0)], \"\n",
    "    \"RUSSIAN_MOVIE=[MIN($1)])\"\n",
    ")\n",
    "input_cols = ['uncited_voiced_character', 'russian_movie']\n",
    "\n",
    "group_cols, mapping = parse_bindable_aggregate(line, input_cols)\n",
    "\n",
    "# Now you can do:\n",
    "#    df.groupby(group_cols).agg(mapping)\n",
    "#\n",
    "# For this example, since group_cols==[],\n",
    "# it becomes:\n",
    "#    df.agg(mapping)\n",
    "\n",
    "print(\"group by:\", group_cols)\n",
    "print(\"agg map:\", mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a3223a4-0b9a-45dc-937e-81d1915095e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "{'SUM_VALUE': ('value', 'sum'), 'MIN_SCORE': ('score', 'min')}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SUM_VALUE</th>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIN_SCORE</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value  score\n",
       "SUM_VALUE  150.0    NaN\n",
       "MIN_SCORE    NaN    2.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the parser function\n",
    "import re\n",
    "def parse_bindable_aggregate(line, input_cols):\n",
    "    grp_match = re.search(r'group=\\[\\{(.*?)\\}\\]', line)\n",
    "    group_cols = [c.strip() for c in grp_match.group(1).split(',')] if grp_match and grp_match.group(1).strip() else []\n",
    "    pairs = re.findall(r'(\\w+)=\\[\\s*(\\w+)\\s*\\(\\s*\\$(\\d+)\\)\\s*\\]', line)\n",
    "    return group_cols, {out_col: (input_cols[int(idx)], func.lower()) for out_col, func, idx in pairs}\n",
    "\n",
    "# Create example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'category': ['A', 'A', 'B', 'B', 'C'],\n",
    "    'id': [1, 2, 3, 4, 5],\n",
    "    'value': [10, 20, 30, 40, 50],\n",
    "    'score': [5, 3, 6, 2, 4]\n",
    "})\n",
    "\n",
    "# Define BindableAggregate line and input_cols\n",
    "line = \"group=[{}], SUM_VALUE=[SUM($2)], MIN_SCORE=[MIN($3)]\"\n",
    "input_cols = ['category', 'id', 'value', 'score']\n",
    "\n",
    "# Parse to get group columns and aggregation mapping\n",
    "group_cols, agg_map = parse_bindable_aggregate(line, input_cols)\n",
    "\n",
    "print(group_cols)\n",
    "print(agg_map)\n",
    "\n",
    "if group_cols:\n",
    "    # regular GROUP BY\n",
    "    result = (\n",
    "        df\n",
    "        .groupby(group_cols)\n",
    "        .agg(**agg_map)\n",
    "        .reset_index()\n",
    "    )\n",
    "else:\n",
    "    # no GROUP BY → aggregate entire frame\n",
    "    result = df.agg(**agg_map)\n",
    "\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d0f43-0c84-44fd-a0d7-591e3761ed1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
